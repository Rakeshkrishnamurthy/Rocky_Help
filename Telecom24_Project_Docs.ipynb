{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+uAOdvNOz4/G3a9XFJR78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakeshkrishnamurthy/Rocky_Help/blob/main/Telecom24_Project_Docs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) First creating of schema and parameters tables in hive metastore***"
      ],
      "metadata": {
        "id": "9GElFDUUX4W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***metadata_table_creation_scrept***"
      ],
      "metadata": {
        "id": "l0FfLYn3TVSw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq4nwNlsTOs7"
      },
      "outputs": [],
      "source": [
        "#Creating parameter for Storage account\n",
        "dbutils.widgets.text(\"storage_account_name\", \"\")\n",
        "storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
        "\n",
        "#Creating SQL DB for Schema\n",
        "%sql\n",
        "create database if not exists hive_metastore.metadata_schema\n",
        "\n",
        "#tbl_source_control metadata table holds the parameters values for various connections\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table metadata_schema.tbl_source_control (\n",
        "\n",
        "    gcp_source_bucket string,\n",
        "    storage_account string,\n",
        "    adls_url string,\n",
        "    container_name string,\n",
        "    logic_app_url string,\n",
        "    email_id string\n",
        ")\n",
        "location 'abfss://metadata@{storage_account_name}.dfs.core.windows.net/tbl_source_control'\n",
        "\"\"\")\n",
        "\n",
        "# tbl_parameters holds the parameter values of DDL\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table metadata_schema.tbl_parameters (\n",
        "\n",
        "    job_id int,\n",
        "    source_file_path string,\n",
        "    watermark_column timestamp,\n",
        "    raw_schema string,\n",
        "    raw_tbl string,\n",
        "    intermediate_schema string,\n",
        "    intermediate_tbl string,\n",
        "    intermediate_query string,\n",
        "    curated_schema string,\n",
        "    curated_tbl string,\n",
        "    curated_query string\n",
        ")\n",
        "location 'abfss://metadata@{storage_account_name}.dfs.core.windows.net/tbl_parameters'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***raw_tbl_creation***"
      ],
      "metadata": {
        "id": "srwdzc5FUCbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parameting for getting environment and storage account for RAW LAYER\n",
        "dbutils.widgets.text(\"storage_account_name\", \"\")\n",
        "storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
        "\n",
        "dbutils.widgets.text(\"env\", \"\")\n",
        "env = dbutils.widgets.get(\"env\")\n",
        "\n",
        "#Creating the data base for raw\n",
        "spark.sql(f\"create database if not exists hive_metastore.{env}_raw\")\n",
        "\n",
        "#creating dimention table of CITIES in raw layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_raw.dim_cities (\n",
        "\n",
        "    city_code int,\n",
        "    city_name string,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://raw@{storage_account_name}.dfs.core.windows.net/dim_cities'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of date in raw layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_raw.dim_date (\n",
        "\n",
        "    dated string,\n",
        "    month_name string,\n",
        "    before_or_after_5g string,\n",
        "    time_period int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "location 'abfss://raw@{storage_account_name}.dfs.core.windows.net/dim_date'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of plan in raw layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_raw.dim_plan (\n",
        "\n",
        "    plan string,\n",
        "    plan_description string,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "location 'abfss://raw@{storage_account_name}.dfs.core.windows.net/dim_plan'\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "#Fact Tables for metrics_share\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_raw.fact_metrics_share (\n",
        "\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    company string,\n",
        "    company_revenue_crores double,\n",
        "    arpu int,\n",
        "    active_users_lakhs double,\n",
        "    unsubscribed_users_lakhs double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://raw@{storage_account_name}.dfs.core.windows.net/fact_metrics_share'\n",
        "\"\"\")\n",
        "\n",
        "#Fact Table for Market_share\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_raw.fact_market_share (\n",
        "\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    tmv_city_crores double,\n",
        "    company string,\n",
        "    ms_pct double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://raw@{storage_account_name}.dfs.core.windows.net/fact_market_share'\n",
        "\"\"\")\n",
        "\n",
        "#Fact table plan_revenue\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_raw.fact_plan_revenue (\n",
        "\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    plan string,\n",
        "    plan_revenue_crores double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://raw@{storage_account_name}.dfs.core.windows.net/fact_plan_revenue'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "iuDFDeelUE7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***log_table_creation_screpts***"
      ],
      "metadata": {
        "id": "uOShfKyrVYcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parameting for getting environment and storage account\n",
        "dbutils.widgets.text(\"storage_account_name\", \"\")\n",
        "storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
        "dbutils.widgets.text(\"env\", \"\")\n",
        "env = dbutils.widgets.get(\"env\")\n",
        "\n",
        "#Creating the data base for raw\n",
        "spark.sql(f\"\"\"\n",
        "create database if not exists hive_metastore.{env}_log\n",
        "\"\"\")\n",
        "\n",
        "#Log recode table\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_log.log_record_tbl (\n",
        "    env string,\n",
        "    pipeLineName string,\n",
        "    logMessage string,\n",
        "    status string,\n",
        "    triggerType string,\n",
        "    loadId string,\n",
        "    logTimeStamp timestamp\n",
        ")\n",
        "location 'abfss://log@{storage_account_name}.dfs.core.windows.net/log_record_tbl'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "U4FIOJt2Vd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***intermediate_tables_creation_scripts***"
      ],
      "metadata": {
        "id": "mQvoUob8V6Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parameting for getting environment and storage account for INTERMEDIATE\n",
        "dbutils.widgets.text(\"storage_account_name\", \"\")\n",
        "storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
        "\n",
        "dbutils.widgets.text(\"env\", \"\")\n",
        "env = dbutils.widgets.get(\"env\")\n",
        "\n",
        "#Creating Sql DB\n",
        "spark.sql(f\"create database if not exists hive_metastore.{env}_intermediate\")\n",
        "\n",
        "#creating dimention table of cities in intermediate layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_intermediate.dim_cities (\n",
        "\n",
        "    city_code int,\n",
        "    city_name string,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://intermediate@{storage_account_name}.dfs.core.windows.net/dim_cities'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of date in intermediate layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_intermediate.dim_date (\n",
        "\n",
        "    dated string,\n",
        "    month_name string,\n",
        "    before_or_after_5g string,\n",
        "    time_period int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "location 'abfss://intermediate@{storage_account_name}.dfs.core.windows.net/dim_date'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of plan in intermediate layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_intermediate.dim_plan (\n",
        "\n",
        "    plan string,\n",
        "    plan_description string,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "location 'abfss://intermediate@{storage_account_name}.dfs.core.windows.net/dim_plan'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of metrics_share in intermediate layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_intermediate.fact_metrics_share (\n",
        "\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    company string,\n",
        "    company_revenue_crores double,\n",
        "    arpu int,\n",
        "    active_users_lakhs double,\n",
        "    unsubscribed_users_lakhs double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://intermediate@{storage_account_name}.dfs.core.windows.net/fact_metrics_share'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of market_share in intermediate layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_intermediate.fact_market_share (\n",
        "\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    tmv_city_crores double,\n",
        "    company string,\n",
        "    ms_pct double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://intermediate@{storage_account_name}.dfs.core.windows.net/fact_market_share'\n",
        "\"\"\")\n",
        "\n",
        "#creating dimention table of plan_revenue in intermediate layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_intermediate.fact_plan_revenue (\n",
        "\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    plan string,\n",
        "    plan_revenue_crores double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://intermediate@{storage_account_name}.dfs.core.windows.net/fact_plan_revenue'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "iNa2SH8EV68V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**curated_tables_creation_scripts**"
      ],
      "metadata": {
        "id": "Xc1G6k1KW3oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parameting for getting environment and storage account for CURATED\n",
        "dbutils.widgets.text(\"storage_account_name\", \"\")\n",
        "storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
        "\n",
        "dbutils.widgets.text(\"env\", \"\")\n",
        "env = dbutils.widgets.get(\"env\")\n",
        "\n",
        "#Creation of SQL DB\n",
        "spark.sql(f\"create database if not exists hive_metastore.{env}_curated\")\n",
        "\n",
        "#creating of table metrics_share in CURATED layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_curated.metrics_share (\n",
        "\n",
        "    before_or_after_5g string,\n",
        "    city_name string,\n",
        "    month_name string,\n",
        "    time_period int,\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    company string,\n",
        "    company_revenue_crores double,\n",
        "    arpu int,\n",
        "    active_users_lakhs double,\n",
        "    unsubscribed_users_lakhs double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://curated@{storage_account_name}.dfs.core.windows.net/metrics_share'\n",
        "\"\"\")\n",
        "\n",
        "#creating of table metrics_share in CURATED layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_curated.market_share (\n",
        "\n",
        "    before_or_after_5g string,\n",
        "    city_name string,\n",
        "    month_name string,\n",
        "    time_period int,\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    tmv_city_crores double,\n",
        "    company string,\n",
        "    ms_pct double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://curated@{storage_account_name}.dfs.core.windows.net/market_share'\n",
        "\"\"\")\n",
        "\n",
        "#creating of table metrics_share in CURATED layer\n",
        "spark.sql(f\"\"\"\n",
        "create or replace table {env}_curated.plan_revenue (\n",
        "\n",
        "    before_or_after_5g string,\n",
        "    city_name string,\n",
        "    month_name string,\n",
        "    time_period int,\n",
        "    plan_description string,\n",
        "    dated string,\n",
        "    city_code int,\n",
        "    plan string,\n",
        "    plan_revenue_crores double,\n",
        "    seq_no int,\n",
        "    last_inserted_dttm_azure timestamp,\n",
        "    last_updated_dttm_gcp string,\n",
        "    load_id string\n",
        ")\n",
        "partitioned by (city_code)\n",
        "location 'abfss://curated@{storage_account_name}.dfs.core.windows.net/plan_revenue'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "un9d6rlYW2BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2) Creation if insertion for all the 3 layers***"
      ],
      "metadata": {
        "id": "t_6cX7HcYxSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***metadata_tables_insert_scripts_till_raw***"
      ],
      "metadata": {
        "id": "uTKb56SlYjb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Paramaeting the detils for data moment from Source to RAW\n",
        "dbutils.widgets.text(\"logic_app_url\", \"\")\n",
        "logic_app_url = dbutils.widgets.get(\"logic_app_url\")\n",
        "\n",
        "dbutils.widgets.text(\"email_id\", \"\")\n",
        "email_id = dbutils.widgets.get(\"email_id\")\n",
        "\n",
        "dbutils.widgets.text(\"storage_account\", \"\")\n",
        "storage_account = dbutils.widgets.get(\"storage_account\")\n",
        "\n",
        "dbutils.widgets.text(\"adls_url\", \"\")\n",
        "adls_url = dbutils.widgets.get(\"adls_url\")\n",
        "\n",
        "dbutils.widgets.text(\"gcp_bucket\",\"\")\n",
        "gcp_bucket = dbutils.widgets.get(\"gcp_bucket\")\n",
        "\n",
        "dbutils.widgets.text(\"gcp_bucket\",\"\")\n",
        "gcp_bucket = dbutils.widgets.get(\"gcp_bucket\")\n",
        "\n",
        "\n",
        "#Insertion to the sorce table by passing paramaetrs\n",
        "spark.sql(f\"\"\"\n",
        "insert into metadata_schema.tbl_source_control values\n",
        "('{gcp_bucket}','{storage_account}','{adls_url}','landing','{logic_app_url}','{email_id}')\n",
        "\"\"\")\n",
        "\n",
        "#initail inertion of data to the source\n",
        "spark.sql(\"\"\"\n",
        "insert into metadata_schema.tbl_parameters values\n",
        "(201,'telecom/dim_cities','2001-01-01T15:13:23.963Z','dev_raw','dim_cities','dev_intermediate','dim_cities',NULL,NULL,NULL,NULL),\n",
        "(202,'telecom/dim_date','2001-01-01T15:13:23.963Z','dev_raw','dim_date','dev_intermediate','dim_date',NULL,NULL,NULL,NULL),\n",
        "(203,'telecom/dim_plan','2001-01-01T15:13:23.963Z','dev_raw','dim_plan','dev_intermediate','dim_plan',NULL,NULL,NULL,NULL),\n",
        "(204,'telecom/fact_company','2001-01-01T15:13:23.963Z','dev_raw','fact_metrics_share','dev_intermediate','fact_metrics_share',NULL,NULL,NULL,NULL),\n",
        "(205,'telecom/fact_market','2001-01-01T15:13:23.963Z','dev_raw','fact_market_share','dev_intermediate','fact_market_share',NULL,NULL,NULL,NULL),\n",
        "(206,'telecom/fact_plan','2001-01-01T15:13:23.963Z','dev_raw','fact_plan_revenue','dev_intermediate','fact_plan_revenue',NULL,NULL,NULL,NULL)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "TigVo98gYheP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metadata_tables_insert_scripts_till_intermediate"
      ],
      "metadata": {
        "id": "5MF_CGuvZgrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Paramaeting the detils for data moment from Source to Intermediate\n",
        "dbutils.widgets.text(\"logic_app_url\", \"\")\n",
        "logic_app_url = dbutils.widgets.get(\"logic_app_url\")\n",
        "\n",
        "dbutils.widgets.text(\"email_id\", \"\")\n",
        "email_id = dbutils.widgets.get(\"email_id\")\n",
        "\n",
        "dbutils.widgets.text(\"storage_account\", \"\")\n",
        "storage_account = dbutils.widgets.get(\"storage_account\")\n",
        "\n",
        "dbutils.widgets.text(\"adls_url\", \"\")\n",
        "adls_url = dbutils.widgets.get(\"adls_url\")\n",
        "\n",
        "dbutils.widgets.text(\"gcp_bucket\",\"\")\n",
        "gcp_bucket = dbutils.widgets.get(\"gcp_bucket\")\n",
        "\n",
        "dbutils.widgets.text(\"gcp_bucket\",\"\")\n",
        "gcp_bucket = dbutils.widgets.get(\"gcp_bucket\")\n",
        "\n",
        "#Insertion to intermediet from the sorce table by passing parametrs\n",
        "spark.sql(f\"\"\"\n",
        "\n",
        "insert into metadata_schema.tbl_source_control values\n",
        "('{gcp_bucket}','{storage_account}','{adls_url}','landing','{logic_app_url}','{email_id}')\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "#Full load and delta or Incremental load to Intermidate\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "insert into metadata_schema.tbl_parameters values\n",
        "(201,'telecom/dim_cities','2001-01-01T15:13:23.963Z','dev_raw','dim_cities','dev_intermediate','dim_cities',NULL,NULL,NULL,NULL),\n",
        "(202,'telecom/dim_date','2001-01-01T15:13:23.963Z','dev_raw','dim_date','dev_intermediate','dim_date',NULL,NULL,NULL,NULL),\n",
        "(203,'telecom/dim_plan','2001-01-01T15:13:23.963Z','dev_raw','dim_plan','dev_intermediate','dim_plan',NULL,NULL,NULL,NULL),\n",
        "(204,'telecom/fact_company','2001-01-01T15:13:23.963Z','dev_raw','fact_metrics_share','dev_intermediate','fact_metrics_share','merge into {intermediate_schema}.{intermediate_table} target\n",
        "  using temp_view source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.company = source.company,\n",
        "      target.company_revenue_crores = source.company_revenue_crores,\n",
        "      target.arpu = source.arpu,\n",
        "      target.active_users_lakhs = source.active_users_lakhs,\n",
        "      target.unsubscribed_users_lakhs = source.unsubscribed_users_lakhs,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = source.load_id\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.company,\n",
        "      target.company_revenue_crores,\n",
        "      target.arpu,\n",
        "      target.active_users_lakhs,\n",
        "      target.unsubscribed_users_lakhs,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.company,\n",
        "      source.company_revenue_crores,\n",
        "      source.arpu,\n",
        "      source.active_users_lakhs,\n",
        "      source.unsubscribed_users_lakhs,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      source.load_id\n",
        "    )',NULL,NULL,NULL),\n",
        "(205,'telecom/fact_market','2001-01-01T15:13:23.963Z','dev_raw','fact_market_share','dev_intermediate','fact_market_share','merge into {intermediate_schema}.{intermediate_table} target\n",
        "  using temp_view source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.tmv_city_crores = source.tmv_city_crores,\n",
        "      target.company = source.company,\n",
        "      target.ms_pct = source.ms_pct,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = source.load_id\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.tmv_city_crores,\n",
        "      target.company,\n",
        "      target.ms_pct,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.tmv_city_crores,\n",
        "      source.company,\n",
        "      source.ms_pct,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      source.load_id\n",
        "    )',NULL,NULL,NULL),\n",
        "(206,'telecom/fact_plan','2001-01-01T15:13:23.963Z','dev_raw','fact_plan_revenue','dev_intermediate','fact_plan_revenue','merge into {intermediate_schema}.{intermediate_table} target\n",
        "  using temp_view source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.plan = source.plan,\n",
        "      target.plan_revenue_crores = source.plan_revenue_crores,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = source.load_id\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.plan,\n",
        "      target.plan_revenue_crores,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.plan,\n",
        "      source.plan_revenue_crores,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      source.load_id\n",
        "    )',NULL,NULL,NULL)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "sBw2iSvgZhUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metadata_tables_insert_scripts_till_curated"
      ],
      "metadata": {
        "id": "SkxmC1_5azmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Paramaeting the detils for data moment from Source to Intermediate\n",
        "dbutils.widgets.text(\"logic_app_url\", \"\")\n",
        "logic_app_url = dbutils.widgets.get(\"logic_app_url\")\n",
        "\n",
        "dbutils.widgets.text(\"email_id\", \"\")\n",
        "email_id = dbutils.widgets.get(\"email_id\")\n",
        "\n",
        "dbutils.widgets.text(\"storage_account\", \"\")\n",
        "storage_account = dbutils.widgets.get(\"storage_account\")\n",
        "\n",
        "dbutils.widgets.text(\"adls_url\", \"\")\n",
        "adls_url = dbutils.widgets.get(\"adls_url\")\n",
        "\n",
        "dbutils.widgets.text(\"gcp_bucket\",\"\")\n",
        "gcp_bucket = dbutils.widgets.get(\"gcp_bucket\")\n",
        "\n",
        "dbutils.widgets.text(\"gcp_bucket\",\"\")\n",
        "gcp_bucket = dbutils.widgets.get(\"gcp_bucket\")\n",
        "\n",
        "##Insertion to curated from the intermidate by passing parametrs\n",
        "spark.sql(f\"\"\"\n",
        "\n",
        "insert into metadata_schema.tbl_source_control values\n",
        "('{gcp_bucket}','{storage_account}','{adls_url}','landing','{logic_app_url}','{email_id}')\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "#Full load and incremental load from Intermidate to Curated\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "insert into metadata_schema.tbl_parameters values\n",
        "(201,'telecom/dim_cities','2001-01-01T15:13:23.963Z','dev_raw','dim_cities','dev_intermediate','dim_cities',NULL,NULL,NULL,NULL),\n",
        "(202,'telecom/dim_date','2001-01-01T15:13:23.963Z','dev_raw','dim_date','dev_intermediate','dim_date',NULL,NULL,NULL,NULL),\n",
        "(203,'telecom/dim_plan','2001-01-01T15:13:23.963Z','dev_raw','dim_plan','dev_intermediate','dim_plan',NULL,NULL,NULL,NULL),\n",
        "(204,'telecom/fact_company','2001-01-01T15:13:23.963Z','dev_raw','fact_metrics_share','dev_intermediate','fact_metrics_share','merge into {intermediate_schema}.{intermediate_table} target\n",
        "  using temp_view source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.company = source.company,\n",
        "      target.company_revenue_crores = source.company_revenue_crores,\n",
        "      target.arpu = source.arpu,\n",
        "      target.active_users_lakhs = source.active_users_lakhs,\n",
        "      target.unsubscribed_users_lakhs = source.unsubscribed_users_lakhs,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = source.load_id\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.company,\n",
        "      target.company_revenue_crores,\n",
        "      target.arpu,\n",
        "      target.active_users_lakhs,\n",
        "      target.unsubscribed_users_lakhs,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.company,\n",
        "      source.company_revenue_crores,\n",
        "      source.arpu,\n",
        "      source.active_users_lakhs,\n",
        "      source.unsubscribed_users_lakhs,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      source.load_id\n",
        "    )',NULL,NULL,NULL),\n",
        "(205,'telecom/fact_market','2001-01-01T15:13:23.963Z','dev_raw','fact_market_share','dev_intermediate','fact_market_share','merge into {intermediate_schema}.{intermediate_table} target\n",
        "  using temp_view source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.tmv_city_crores = source.tmv_city_crores,\n",
        "      target.company = source.company,\n",
        "      target.ms_pct = source.ms_pct,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = source.load_id\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.tmv_city_crores,\n",
        "      target.company,\n",
        "      target.ms_pct,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.tmv_city_crores,\n",
        "      source.company,\n",
        "      source.ms_pct,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      source.load_id\n",
        "    )',NULL,NULL,NULL),\n",
        "(206,'telecom/fact_plan','2001-01-01T15:13:23.963Z','dev_raw','fact_plan_revenue','dev_intermediate','fact_plan_revenue','merge into {intermediate_schema}.{intermediate_table} target\n",
        "  using temp_view source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.plan = source.plan,\n",
        "      target.plan_revenue_crores = source.plan_revenue_crores,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = source.load_id\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.plan,\n",
        "      target.plan_revenue_crores,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.plan,\n",
        "      source.plan_revenue_crores,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      source.load_id\n",
        "    )',NULL,NULL,NULL),\n",
        "    (207,NULL,NULL,NULL,NULL,'dev_intermediate',NULL,NULL,'dev_curated','metrics_share',\"with result as (\n",
        "  select\n",
        "    dd.before_or_after_5g,\n",
        "    cities.city_name,\n",
        "    dd.month_name,\n",
        "    dd.time_period,\n",
        "    metrix.*\n",
        "  from\n",
        "    {intermediate_schema}.fact_metrics_share as metrix\n",
        "    join {intermediate_schema}.dim_date as dd on metrix.dated = dd.dated\n",
        "    join {intermediate_schema}.dim_cities as cities on metrix.city_code = cities.city_code\n",
        ")\n",
        "merge into {curated_schema}.{curated_table} as target\n",
        "  using result as source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.before_or_after_5g = source.before_or_after_5g,\n",
        "      target.city_name = source.city_name,\n",
        "      target.month_name = source.month_name,\n",
        "      target.time_period = source.time_period,\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.company = source.company,\n",
        "      target.company_revenue_crores = source.company_revenue_crores,\n",
        "      target.arpu = source.arpu,\n",
        "      target.active_users_lakhs = source.active_users_lakhs,\n",
        "      target.unsubscribed_users_lakhs = source.unsubscribed_users_lakhs,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = '{LoadID}'\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.before_or_after_5g,\n",
        "      target.city_name,\n",
        "      target.month_name,\n",
        "      target.time_period,\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.company,\n",
        "      target.company_revenue_crores,\n",
        "      target.arpu,\n",
        "      target.active_users_lakhs,\n",
        "      target.unsubscribed_users_lakhs,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.before_or_after_5g,\n",
        "      source.city_name,\n",
        "      source.month_name,\n",
        "      source.time_period,\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.company,\n",
        "      source.company_revenue_crores,\n",
        "      source.arpu,\n",
        "      source.active_users_lakhs,\n",
        "      source.unsubscribed_users_lakhs,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      \\'{LoadID}\\'\n",
        "    )\"),\n",
        "    (208,NULL,NULL,NULL,NULL,'dev_intermediate',NULL,NULL,'dev_curated','market_share',\"with result as (\n",
        "  select\n",
        "    dd.before_or_after_5g,\n",
        "    cities.city_name,\n",
        "    dd.month_name,\n",
        "    dd.time_period,\n",
        "    market.*\n",
        "  from\n",
        "    {intermediate_schema}.fact_market_share as market\n",
        "    join {intermediate_schema}.dim_date as dd on market.dated = dd.dated\n",
        "    join {intermediate_schema}.dim_cities as cities on market.city_code = cities.city_code\n",
        ")\n",
        "merge into {curated_schema}.{curated_table} as target\n",
        "  using result as source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.before_or_after_5g = source.before_or_after_5g,\n",
        "      target.city_name = source.city_name,\n",
        "      target.month_name = source.month_name,\n",
        "      target.time_period = source.time_period,\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.tmv_city_crores = source.tmv_city_crores,\n",
        "      target.company = source.company,\n",
        "      target.ms_pct = source.ms_pct,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = \\'{LoadID}\\'\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.before_or_after_5g,\n",
        "      target.city_name,\n",
        "      target.month_name,\n",
        "      target.time_period,\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.tmv_city_crores,\n",
        "      target.company,\n",
        "      target.ms_pct,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.before_or_after_5g,\n",
        "      source.city_name,\n",
        "      source.month_name,\n",
        "      source.time_period,\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.tmv_city_crores,\n",
        "      source.company,\n",
        "      source.ms_pct,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      \\'{LoadID}\\'\n",
        "    )\"),\n",
        "    (209,NULL,NULL,NULL,NULL,'dev_intermediate',NULL,NULL,'dev_curated','plan_revenue',\"with result as (\n",
        "  select\n",
        "    dd.before_or_after_5g,\n",
        "    cities.city_name,\n",
        "    dd.month_name,\n",
        "    dd.time_period,\n",
        "    pl.plan_description,\n",
        "    revenue.*\n",
        "  from\n",
        "    {intermediate_schema}.fact_plan_revenue as revenue\n",
        "    join {intermediate_schema}.dim_date as dd on revenue.dated = dd.dated\n",
        "    join {intermediate_schema}.dim_cities as cities on revenue.city_code = cities.city_code\n",
        "    join {intermediate_schema}.dim_plan pl on revenue.plan = pl.plan\n",
        ")\n",
        "merge into {curated_schema}.{curated_table} as target\n",
        "  using result as source\n",
        "  on\n",
        "    source.seq_no = target.seq_no\n",
        "  When matched then\n",
        "    Update set\n",
        "      target.before_or_after_5g = source.before_or_after_5g,\n",
        "      target.city_name = source.city_name,\n",
        "      target.month_name = source.month_name,\n",
        "      target.time_period = source.time_period,\n",
        "      target.plan_description = source.plan_description,\n",
        "      target.dated = source.dated,\n",
        "      target.city_code = source.city_code,\n",
        "      target.plan = source.plan,\n",
        "      target.plan_revenue_crores = source.plan_revenue_crores,\n",
        "      target.seq_no = source.seq_no,\n",
        "      target.last_inserted_dttm_azure = source.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp = source.last_updated_dttm_gcp,\n",
        "      target.load_id = \\'{LoadID}\\'\n",
        "  when not matched then\n",
        "    insert (\n",
        "      target.before_or_after_5g,\n",
        "      target.city_name,\n",
        "      target.month_name,\n",
        "      target.time_period,\n",
        "      target.plan_description,\n",
        "      target.dated,\n",
        "      target.city_code,\n",
        "      target.plan,\n",
        "      target.plan_revenue_crores,\n",
        "      target.seq_no,\n",
        "      target.last_inserted_dttm_azure,\n",
        "      target.last_updated_dttm_gcp,\n",
        "      target.load_id\n",
        "    )\n",
        "    values (\n",
        "      source.before_or_after_5g,\n",
        "      source.city_name,\n",
        "      source.month_name,\n",
        "      source.time_period,\n",
        "      source.plan_description,\n",
        "      source.dated,\n",
        "      source.city_code,\n",
        "      source.plan,\n",
        "      source.plan_revenue_crores,\n",
        "      source.seq_no,\n",
        "      source.last_inserted_dttm_azure,\n",
        "      source.last_updated_dttm_gcp,\n",
        "      '{LoadID}'\n",
        "    )\n",
        "\")\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "yIF9SgFHa-M3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}