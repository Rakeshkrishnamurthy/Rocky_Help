{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRJgef85IfP1BllbRxk9RA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakeshkrishnamurthy/Rocky_Help/blob/main/Raw_Staging_TGT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQakve895JbK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Step 1: Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DataProcessingAndTransformation\").getOrCreate()\n",
        "\n",
        "# Step 2: Load raw data and handle null values and duplicates\n",
        "raw_data_path = \"raw/path\"\n",
        "raw_df = spark.read.parquet(raw_data_path)\n",
        "\n",
        "# Handling null values\n",
        "processed_df = raw_df.na.drop()\n",
        "\n",
        "# Handling duplicates\n",
        "processed_df = processed_df.dropDuplicates()\n",
        "\n",
        "#Remove leading and trailing spaces for multiple column\n",
        "columns_to_trim = [\"column1\", \"column2\", \"column3\"]\n",
        "\n",
        "for col_name in columns_to_trim:\n",
        "    clean_data = clean_data.withColumn(col_name, trim(col(col_name)))\n",
        "\n",
        "# Step 3: Load data to staging and perform validation\n",
        "staging_path = \"staging/path/managed\"\n",
        "processed_df.write.parquet(staging_path, mode=\"overwrite\")\n",
        "\n",
        "# Perform data and schema validation on staging data\n",
        "# (Implement your validation logic here)\n",
        "\n",
        "# Step 4: Load data to target and perform transformations\n",
        "target_path = \"target/path/extr\"\n",
        "transformed_df = spark.read.parquet(staging_path)\n",
        "\n",
        "# a) Partition based on year and month\n",
        "transformed_df = transformed_df.withColumn(\"year_month\", col(\"date_column\").substr(1, 7))\n",
        "\n",
        "# b) Split the data in col_info column after 'expo_' and create a new column\n",
        "transformed_df = transformed_df.withColumn(\"new_column\", when(col(\"col_info\").contains(\"expo_\"), col(\"col_info\").substr(6)).otherwise(None))\n",
        "\n",
        "# c) Self join for credit payback information\n",
        "window_spec = Window.partitionBy(\"client_id\").orderBy(\"payment_date\")\n",
        "transformed_df = transformed_df.withColumn(\"previous_payment_date\", lag(\"payment_date\").over(window_spec))\n",
        "\n",
        "# Additional transformations...\n",
        "pliting the single column and creating new two columns\n",
        "\n",
        "# Define the list of column names to filter\n",
        "column_names = [\"darshan_key\", \"rakesh_key\", \"mala_key\"]\n",
        "\n",
        "# Filter the DataFrame to include only rows with specified column names\n",
        "filtered_df = df.filter(col(\"name\").isin(column_names) & col(\"name\").like(\"%_key\"))\n",
        "\n",
        "# Perform the required transformations\n",
        "result_df = filtered_df.select(\n",
        "    col(\"t.name\").alias(\"vn_tables\"),\n",
        "    col(\"c.name\").alias(\"vn_name\"),\n",
        "    regexp_replace(col(\"c.name\"), \"_key$\", \"_name\").alias(\"vn_darshan\"),\n",
        "    regexp_replace(col(\"c.name\"), \"_key$\", \"\").alias(\"orthogonal\")\n",
        ")\n",
        "\n",
        "\n",
        "# Step 5: Save the output in Parquet format to production location\n",
        "output_path = \"target/path/prod\"\n",
        "transformed_df.write.parquet(output_path, mode=\"overwrite\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "NJE2Ecmg5KKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2KwqZE35TXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, lag\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Step 1: Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"IncrementalDataProcessing\").getOrCreate()\n",
        "\n",
        "# Step 2: Load raw data and handle null values and duplicates\n",
        "raw_data_path = \"raw/path\"\n",
        "raw_df = spark.read.parquet(raw_data_path)\n",
        "\n",
        "# Handling null values\n",
        "processed_df = raw_df.na.drop()\n",
        "\n",
        "# Handling duplicates\n",
        "processed_df = processed_df.dropDuplicates()\n",
        "\n",
        "# Step 3: Load data to staging and perform validation\n",
        "staging_path = \"staging/path/managed\"\n",
        "processed_df.write.mode(\"append\").parquet(staging_path)\n",
        "\n",
        "# Perform data and schema validation on staging data\n",
        "# (Implement your validation logic here)\n",
        "\n",
        "# Step 4: Load data to target and perform transformations\n",
        "target_path = \"target/path/extr\"\n",
        "existing_data = spark.read.parquet(target_path)\n",
        "\n",
        "# Identify new or updated records\n",
        "processed_df = processed_df.join(existing_data, [\"common_key_column\"], \"leftanti\")\n",
        "\n",
        "# a) Partition based on year and month\n",
        "processed_df = processed_df.withColumn(\"year_month\", col(\"date_column\").substr(1, 7))\n",
        "\n",
        "# b) Split the data in col_info column after 'expo_' and create a new column\n",
        "processed_df = processed_df.withColumn(\"new_column\", when(col(\"col_info\").contains(\"expo_\"), col(\"col_info\").substr(6)).otherwise(None))\n",
        "\n",
        "# c) Self join for credit payback information\n",
        "window_spec = Window.partitionBy(\"client_id\").orderBy(\"payment_date\")\n",
        "processed_df = processed_df.withColumn(\"previous_payment_date\", lag(\"payment_date\").over(window_spec))\n",
        "\n",
        "# Additional transformations...\n",
        "# (Implement your specific transformations here)\n",
        "\n",
        "# Step 5: Save the output in Parquet format to production location\n",
        "processed_df.write.mode(\"append\").parquet(target_path)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "THhAhIcx5Tt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}